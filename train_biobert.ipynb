{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train biobert.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAOJcOLOwFI7"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EeomMfO-FcB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import itertools\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange\n",
        "from collections import defaultdict, OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "from pytorch_pretrained_bert import BertModel, BertTokenizer, BertConfig\n",
        "from transformers import BertForTokenClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "2ZcKWjBbwKFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get GPU device name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name == '/device:GPU:0':\n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "  raise SystemError('GPU device not found')\n",
        "  "
      ],
      "metadata": {
        "id": "piNSUG5xweiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tell Pytorch to use the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "print('We will use the GPU:', torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "iETbX4Dbwk2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\" -O biobert_weights && rm -rf /tmp/cookies.txt\n"
      ],
      "metadata": {
        "id": "CZrWWKzowrwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf biobert_weights\n",
        "!ls biobert_v1.1_pubmed/"
      ],
      "metadata": {
        "id": "Un5cgRTFwwos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.mkdir('/content/drive/MyDrive/biobert_v1.1_pubmed')\n"
      ],
      "metadata": {
        "id": "kV70yBC7pePA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for files in os.listdir('/content/biobert_v1.1_pubmed/'):\n",
        "  shutil.copy(os.path.join('/content/biobert_v1.1_pubmed/', files), \"/content/drive/MyDrive/biobert_v1.1_pubmed\")"
      ],
      "metadata": {
        "id": "bvldNdELpk6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copy('/content/biobert_v1.1_pubmed/', \"/content/drive/MyDrive\")"
      ],
      "metadata": {
        "id": "wrIE2ILIn5j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!transformers-cli convert --model_type bert --tf_checkpoint biobert_v1.1_pubmed/model.ckpt-1000000 --config biobert_v1.1_pubmed/bert_config.json --pytorch_dump_output biobert_v1.1_pubmed/pytorch_model.bin\n"
      ],
      "metadata": {
        "id": "Ck2slUXjw4Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls biobert_v1.1_pubmed/\n",
        "!mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json\n",
        "!ls biobert_v1.1_pubmed/"
      ],
      "metadata": {
        "id": "6WJEGcv5w8Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls "
      ],
      "metadata": {
        "id": "qRr8EmpExGka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 75\n",
        "BATCH_SIZE = 32\n",
        "tokenizer = BertTokenizer(vocab_file='biobert_v1.1_pubmed/vocab.txt', do_lower_case=False)"
      ],
      "metadata": {
        "id": "aFJyurZRxIws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/BioBERT/tags.csv')\n",
        "tag_values = data['tags'].values\n",
        "vocab_len = len(tag_values)\n",
        "print('Entity Types:',vocab_len)"
      ],
      "metadata": {
        "id": "Zx4pqaaf003B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tags = pd.DataFrame({'tags':tag_values})\n",
        "df_tags.to_csv('bionlp_tags.csv',index=False)\n",
        "df = pd.read_csv('bionlp_tags.csv')\n",
        "print('Tag Preview:\\n', df)"
      ],
      "metadata": {
        "id": "qjHckfAb1BfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceFetch(object):\n",
        "  \n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "    self.sentences = []\n",
        "    self.tags = []\n",
        "    self.sent = []\n",
        "    self.tag = []\n",
        "    \n",
        "    # make tsv file readable\n",
        "    with open(self.data) as tsv_f:\n",
        "      reader = csv.reader(tsv_f, delimiter='\\t')\n",
        "      for row in reader:\n",
        "        if len(row) == 0:\n",
        "          if len(self.sent) != len(self.tag):\n",
        "            break\n",
        "          self.sentences.append(self.sent)\n",
        "          self.tags.append(self.tag)\n",
        "          self.sent = []\n",
        "          self.tag = []\n",
        "        else:\n",
        "          self.sent.append(row[0])\n",
        "          self.tag.append(row[1])   \n",
        "\n",
        "  def getSentences(self):\n",
        "    return self.sentences\n",
        "  \n",
        "  def getTags(self):\n",
        "    return self.tags"
      ],
      "metadata": {
        "id": "_BukSCB31D8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpora = '//content/drive/MyDrive/BioBERT/BioNLP'\n",
        "sentences = []\n",
        "tags = []\n",
        "for subdir, dirs, files in os.walk(corpora):\n",
        "    for file in files:\n",
        "        if file == 'train.tsv':\n",
        "            path = os.path.join(subdir, file)\n",
        "            sent = SentenceFetch(path).getSentences()\n",
        "            tag = SentenceFetch(path).getTags()\n",
        "            sentences.extend(sent)\n",
        "            tags.extend(tag)\n",
        "            \n",
        "sentences = sentences[0:20000]\n",
        "tags = tags[0:20000]"
      ],
      "metadata": {
        "id": "Zwuw3HkG2N0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Sentence Preview:\\n',sentences[0])"
      ],
      "metadata": {
        "id": "oMdAuB792STQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tok_with_labels(sent, text_labels):\n",
        "  '''tokenize and keep labels intact'''\n",
        "  tok_sent = []\n",
        "  labels = []\n",
        "  for word, label in zip(sent, text_labels):\n",
        "    tok_word = tokenizer.tokenize(word)\n",
        "    n_subwords = len(tok_word)\n",
        "\n",
        "    tok_sent.extend(tok_word)\n",
        "    labels.extend([label] * n_subwords)\n",
        "  return tok_sent, labels\n",
        "\n",
        "tok_texts_and_labels = [tok_with_labels(sent, labs) for sent, labs in zip(sentences, tags)]"
      ],
      "metadata": {
        "id": "uZi6Gmba2wAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok_texts = [tok_label_pair[0] for tok_label_pair in tok_texts_and_labels]\n",
        "labels = [tok_label_pair[1] for tok_label_pair in tok_texts_and_labels]"
      ],
      "metadata": {
        "id": "5hL7ypP93ckp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tok_texts)"
      ],
      "metadata": {
        "id": "UkYQGJKd3eOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tok_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")"
      ],
      "metadata": {
        "id": "b9YD2UpK3g-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for char in tok_texts:\n",
        "    print('WordPiece Tokenizer Preview:\\n', char)\n",
        "    break"
      ],
      "metadata": {
        "id": "S6fcmaaQ3i4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_values = list(set(itertools.chain.from_iterable(tags)))\n",
        "tag_values.append(\"PAD\")\n",
        "\n",
        "tag2idx = {t: i for i,t in enumerate(tag_values)}"
      ],
      "metadata": {
        "id": "XhlsjzkR3khx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")"
      ],
      "metadata": {
        "id": "g60s2ygz3nIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# attention masks make explicit reference to which tokens are actual words vs padded words\n",
        "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
      ],
      "metadata": {
        "id": "eB3UANgd3owJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "\n",
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "tr_tags = torch.tensor(tr_tags)\n",
        "val_tags = torch.tensor(val_tags)\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ],
      "metadata": {
        "id": "FYXZez9J3qqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "2uClhsPH3tlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = BertConfig.from_json_file('biobert_v1.1_pubmed/config.json')\n",
        "tmp_d = torch.load('biobert_v1.1_pubmed/pytorch_model.bin', map_location=device)\n",
        "state_dict = OrderedDict()\n",
        "\n",
        "for i in list(tmp_d.keys())[:199]:\n",
        "    x = i\n",
        "    if i.find('bert') > -1:\n",
        "        x = '.'.join(i.split('.')[1:])\n",
        "    state_dict[x] = tmp_d[i]"
      ],
      "metadata": {
        "id": "rCvAwYB63v9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BioBertNER(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_len, config, state_dict):\n",
        "    super().__init__()\n",
        "    self.bert = BertModel(config)\n",
        "    self.bert.load_state_dict(state_dict, strict=False)\n",
        "    self.dropout = nn.Dropout(p=0.3)\n",
        "    self.output = nn.Linear(self.bert.config.hidden_size, vocab_len)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    encoded_layer, _ = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    encl = encoded_layer[-1]\n",
        "    out = self.dropout(encl)\n",
        "    out = self.output(out)\n",
        "    return out, out.argmax(-1)"
      ],
      "metadata": {
        "id": "_nV-eKYr3x8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BioBertNER(vocab_len,config,state_dict)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "wuNiQpqt30ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=3e-5,\n",
        "    eps=1e-8\n",
        ")\n",
        "epochs = 3\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "id": "eEwYt24G32ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    for step,batch in enumerate(data_loader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        outputs,y_hat = model(b_input_ids,b_input_mask)\n",
        "        \n",
        "        _,preds = torch.max(outputs,dim=2)\n",
        "        outputs = outputs.view(-1,outputs.shape[-1])\n",
        "        b_labels_shaped = b_labels.view(-1)\n",
        "        loss = loss_fn(outputs,b_labels_shaped)\n",
        "        correct_predictions += torch.sum(preds == b_labels)\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "    return correct_predictions.double()/len(data_loader) , np.mean(losses)"
      ],
      "metadata": {
        "id": "hkcLa6Yd36HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_eval(model,data_loader,loss_fn,device):\n",
        "    model = model.eval()\n",
        "    \n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(data_loader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "            outputs,y_hat = model(b_input_ids,b_input_mask)\n",
        "        \n",
        "            _,preds = torch.max(outputs,dim=2)\n",
        "            outputs = outputs.view(-1,outputs.shape[-1])\n",
        "            b_labels_shaped = b_labels.view(-1)\n",
        "            loss = loss_fn(outputs,b_labels_shaped)\n",
        "            correct_predictions += torch.sum(preds == b_labels)\n",
        "            losses.append(loss.item())\n",
        "        \n",
        "    \n",
        "    return correct_predictions.double()/len(data_loader) , np.mean(losses)"
      ],
      "metadata": {
        "id": "L0aFxNvy38DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "normalizer = BATCH_SIZE*MAX_LEN\n",
        "loss_values = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    total_loss = 0\n",
        "    print(f'======== Epoch {epoch+1}/{epochs} ========')\n",
        "    train_acc,train_loss = train_epoch(model,train_dataloader,loss_fn,optimizer,device,scheduler)\n",
        "    train_acc = train_acc/normalizer\n",
        "    print(f'Train Loss: {train_loss} Train Accuracy: {train_acc}')\n",
        "    total_loss += train_loss.item()\n",
        "    \n",
        "    avg_train_loss = total_loss / len(train_dataloader)  \n",
        "    loss_values.append(avg_train_loss)\n",
        "    \n",
        "    val_acc,val_loss = model_eval(model,valid_dataloader,loss_fn,device)\n",
        "    val_acc = val_acc/normalizer\n",
        "    print(f'Val Loss: {val_loss} Val Accuracy: {val_acc}')\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    \n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)"
      ],
      "metadata": {
        "id": "KX7hNY5L3-h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style='darkgrid')\n",
        "\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# learning curve\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dNvvwwAB4Ajl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save = 'BIONER_classifier.pt'\n",
        "path = F\"{model_save}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "TzmuDK02OrzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"In addition to their essential catalytic role in protein biosynthesis, aminoacyl-tRNA synthetases participate in numerous other functions, including regulation of gene expression and amino acid biosynthesis via transamidation pathways. Herein, we describe a class of aminoacyl-tRNA synthetase-like (HisZ) proteins based on the catalytic core of the contemporary class II histidyl-tRNA synthetase whose members lack aminoacylation activity but are instead essential components of the first enzyme in histidine biosynthesis ATP phosphoribosyltransferase (HisG). Prediction of the function of HisZ in Lactococcus lactis was assisted by comparative genomics, a technique that revealed a link between the presence or the absence of HisZ and a systematic variation in the length of the HisG polypeptide. HisZ is required for histidine prototrophy, and three other lines of evidence support the direct involvement of HisZ in the transferase function. (i) Genetic experiments demonstrate that complementation of an in-frame deletion of HisG from Escherichia coli (which does not possess HisZ) requires both HisG and HisZ from L. lactis. (ii) Coelution of HisG and HisZ during affinity chromatography provides evidence of direct physical interaction. (iii) Both HisG and HisZ are required for catalysis of the ATP phosphoribosyltransferase reaction. This observation of a common protein domain linking amino acid biosynthesis and protein synthesis implies an early connection between the biosynthesis of amino acids and proteins.\"\"\"\n"
      ],
      "metadata": {
        "id": "GQDQ8ZSjVqGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "yXDctubFWCGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_text = nltk.sent_tokenize(text)\n",
        "\n",
        "tokenized_text = []\n",
        "for sentence in sent_text:\n",
        "    tokenized_text.append(nltk.word_tokenize(sentence))\n",
        "\n",
        "def tokenize_and_preserve(sentence):\n",
        "    tokenized_sentence = []\n",
        "    \n",
        "    for word in sentence:\n",
        "        tokenized_word = tokenizer.tokenize(word)   \n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "    return tokenized_sentence\n",
        "\n",
        "tok_texts = [\n",
        "    tokenize_and_preserve(sent) for sent in tokenized_text\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "IHAdcImxVzPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tok_texts]\n",
        "input_attentions = [[1]*len(in_id) for in_id in input_ids]"
      ],
      "metadata": {
        "id": "H0UGJb2-V-uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[1])\n",
        "new_tokens, new_labels = [], []\n",
        "for token in tokens:\n",
        "    if token.startswith(\"##\"):\n",
        "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "    else:\n",
        "        \n",
        "        new_tokens.append(token)"
      ],
      "metadata": {
        "id": "l5r2VxahWoGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_sentences = []\n",
        "pred_labels = []\n",
        "for x,y in zip(input_ids,input_attentions):\n",
        "    x = torch.tensor(x).cuda()\n",
        "    y = torch.tensor(y).cuda()\n",
        "    x = x.view(-1,x.size()[-1])\n",
        "    y = y.view(-1,y.size()[-1])\n",
        "    with torch.no_grad():\n",
        "        _,y_hat = model(x,y)\n",
        "    label_indices = y_hat.to('cpu').numpy()\n",
        "    \n",
        "    tokens = tokenizer.convert_ids_to_tokens(x.to('cpu').numpy()[0])\n",
        "    new_tokens, new_labels = [], []\n",
        "    for token, label_idx in zip(tokens, label_indices[0]):\n",
        "        if token.startswith(\"##\"):\n",
        "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "        else:\n",
        "            new_labels.append(tag_values[label_idx])\n",
        "            new_tokens.append(token)\n",
        "    actual_sentences.append(new_tokens)\n",
        "    pred_labels.append(new_labels)"
      ],
      "metadata": {
        "id": "RLzxgCvVWqRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token, label in zip(actual_sentences, pred_labels):\n",
        "    for t,l in zip(token,label):\n",
        "        print(\"{}\\t{}\".format(t, l))"
      ],
      "metadata": {
        "id": "tWaeO6ObWtkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4TU3-qM9blT_"
      }
    }
  ]
}